**Know what was not tested**

When you're reading an application security report, one of the most important questions that you should get an answer to is what tests did they run? This is especially true of the tests they tried to run but were unsuccessful.

This is because usually in the report, you see the tests that they did that succeeded, and that's only half (or potentially less than half) of what you want to know.

So if you get a report that doesn't have findings, is that because they couldn't get to the application because they: 1) didn't have access to it; 2) because they didn't have coverage; or 3) because there was nothing there?

One of the very important things that you need to see when you look at a security report is what they actually did, particularly what they tried and what didn't work from a security point of view.

In a way, those are actually regression tests -- they're the confirmation that you don't have a particular vulnerability and they also allow you to see how much coverage they have on the application. 

A very good set of questions are thus: How many URLs did they do? How many end point forms did they submit? How many tabs of data did they actually get into?

The answers will give you the information of how good or bad the tests were and how assured you want to be that it will be good.

Imagine you had a building and you had somebody to test its security. If they came along and said, "Well, the building is okay, we couldn't get in," but then you realized that they only covered twenty or thirty or fifty percent of the ports that exist. At that point, you know the quality of the test itself is poor.

In conclusion, I think it's very important that you rate the security assessed on how sure you are of the quality tests. Ultimately, what you want to know is how protected you are from someone who has the time and skill to try every avenue of invasion.
